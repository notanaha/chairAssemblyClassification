{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90206b42",
   "metadata": {},
   "source": [
    "# Step 5 Classifier with Semantic Kernel ResponsesAgent\n",
    "### This script results in an error due to SK’s image handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e14471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"semantic-kernel>=1.27.0\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb626fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, asyncio\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureOpenAISettings\n",
    "from semantic_kernel.contents import (\n",
    "    AuthorRole, ChatMessageContent, TextContent, ImageContent\n",
    ")\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5562bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step5Response(BaseModel):\n",
    "    confidence: int = Field(\n",
    "        description=\"Classification Score (1 means NOT the 5_hexwrench_tight step, 5 means it IS the 5_hexwrench_tight step)\"\n",
    "    )\n",
    "    reasoning: str\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return f\"data:image/jpeg;base64,{base64_encoded}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfbbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_path = Path(\"./system_message_11.txt\")\n",
    "with system_prompt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "items = [\n",
    "    TextContent(text=system_prompt)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85981ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images attached: 10\n"
     ]
    }
   ],
   "source": [
    "# Folder containing sample images (change if needed)\n",
    "sample_folder = \"./output/5_hexwrenchtight/\"\n",
    "\n",
    "#items = [\n",
    "#    TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\")\n",
    "#]\n",
    "items.append(TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\"))\n",
    "\n",
    "# Load every *.jpg in folder\n",
    "image_files = sorted(glob.glob(os.path.join(sample_folder, \"*.jpg\")))\n",
    "for idx, img_path in enumerate(image_files, start=1):\n",
    "    items.append(TextContent(text=f\"sample{idx} – {os.path.basename(img_path)}\"))\n",
    "    items.append(ImageContent(encode_image(img_path)))\n",
    "\n",
    "items.append(TextContent(text=\"The following three images are consecutive frames from a video clip. Based on these images, classify whether they correspond to step 5_hexwrench_tight in the chair assembly process.\"))\n",
    "\n",
    "# 4. Test images\n",
    "test_folder = \"./output/\"\n",
    "test_images = [\n",
    "    \"frame_0063_t63.0s.jpg\",\n",
    "    \"frame_0064_t64.0s.jpg\",\n",
    "    \"frame_0065_t65.0s.jpg\",\n",
    "]\n",
    "\n",
    "for i, img_name in enumerate(test_images, start=1):\n",
    "    img_path = os.path.join(test_folder, img_name)\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Test image {img_path} not found.\")\n",
    "    items.append(TextContent(text=f\"test{i}\"))\n",
    "    items.append(ImageContent(encode_image(img_path)))\n",
    "\n",
    "user_msg = ChatMessageContent(role=AuthorRole.USER, items=items)\n",
    "print(f\"Total images attached: {sum(isinstance(it, ImageContent) for it in items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4be59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Folder containing sample images (change if needed)\\nsample_folder = \"./output/5_hexwrenchtight/\"\\n\\n#items = [\\n#    TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\")\\n#]\\nitems.append(TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\"))\\n\\n# Load every *.jpg in folder\\nimage_files = sorted(glob.glob(os.path.join(sample_folder, \"*.jpg\")))\\nfor idx, img_path in enumerate(image_files, start=1):\\n    items.append(TextContent(text=f\"sample{idx} – {os.path.basename(img_path)}\"))\\n    items.append(ImageContent.from_image_file(img_path))\\n\\nitems.append(TextContent(text=\"The following three images are consecutive frames from a video clip. Based on these images, classify whether they correspond to step 5_hexwrench_tight in the chair assembly process.\"))\\n\\n# 4. Test images\\ntest_folder = \"./output/\"\\ntest_images = [\\n    \"frame_0063_t63.0s.jpg\",\\n    \"frame_0064_t64.0s.jpg\",\\n    \"frame_0065_t65.0s.jpg\",\\n]\\n\\nfor i, img_name in enumerate(test_images, start=1):\\n    img_path = os.path.join(test_folder, img_name)\\n    if not os.path.exists(img_path):\\n        raise FileNotFoundError(f\"Test image {img_path} not found.\")\\n    items.append(TextContent(text=f\"test{i}\"))\\n    items.append(ImageContent.from_image_file(img_path))\\n\\nuser_msg = ChatMessageContent(role=AuthorRole.USER, items=items)\\nprint(f\"Total images attached: {sum(isinstance(it, ImageContent) for it in items)}\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Folder containing sample images (change if needed)\n",
    "sample_folder = \"./output/5_hexwrenchtight/\"\n",
    "\n",
    "#items = [\n",
    "#    TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\")\n",
    "#]\n",
    "items.append(TextContent(text=\"The following images are examples of the 5_hexwrenchtight step in the assembling process of a chair.\"))\n",
    "\n",
    "# Load every *.jpg in folder\n",
    "image_files = sorted(glob.glob(os.path.join(sample_folder, \"*.jpg\")))\n",
    "for idx, img_path in enumerate(image_files, start=1):\n",
    "    items.append(TextContent(text=f\"sample{idx} – {os.path.basename(img_path)}\"))\n",
    "    items.append(ImageContent.from_image_file(img_path))\n",
    "\n",
    "items.append(TextContent(text=\"The following three images are consecutive frames from a video clip. Based on these images, classify whether they correspond to step 5_hexwrench_tight in the chair assembly process.\"))\n",
    "\n",
    "# 4. Test images\n",
    "test_folder = \"./output/\"\n",
    "test_images = [\n",
    "    \"frame_0063_t63.0s.jpg\",\n",
    "    \"frame_0064_t64.0s.jpg\",\n",
    "    \"frame_0065_t65.0s.jpg\",\n",
    "]\n",
    "\n",
    "for i, img_name in enumerate(test_images, start=1):\n",
    "    img_path = os.path.join(test_folder, img_name)\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Test image {img_path} not found.\")\n",
    "    items.append(TextContent(text=f\"test{i}\"))\n",
    "    items.append(ImageContent.from_image_file(img_path))\n",
    "\n",
    "user_msg = ChatMessageContent(role=AuthorRole.USER, items=items)\n",
    "print(f\"Total images attached: {sum(isinstance(it, ImageContent) for it in items)}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3852f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureResponsesAgent.create_client()\n",
    "\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=AzureOpenAISettings().responses_deployment_name,   # <- Responses deployment\n",
    "    client=client,\n",
    "    instructions=\"You are an expert in analysing furniture‑assembly images. \"\n",
    "                 \"Answer in JSON only, following the schema.\",   \n",
    "    name=\"Step5Classifier\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0ab40f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AgentExecutionException",
     "evalue": "(\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'message': 'Too many images in request. Max is 50.', 'type': None, 'param': None, 'code': 'BadRequest'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\semantic_kernel\\agents\\open_ai\\responses_agent_thread_actions.py:495\u001b[0m, in \u001b[0;36mResponsesAgentThreadActions._get_response\u001b[1;34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     response: Response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat_history_for_request(chat_history),\n\u001b[0;32m    497\u001b[0m         instructions\u001b[38;5;241m=\u001b[39mmerged_instructions \u001b[38;5;129;01mor\u001b[39;00m agent\u001b[38;5;241m.\u001b[39minstructions,\n\u001b[0;32m    498\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    499\u001b[0m         store\u001b[38;5;241m=\u001b[39mstore_output_enabled,\n\u001b[0;32m    500\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_options,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:1898\u001b[0m, in \u001b[0;36mAsyncResponses.create\u001b[1;34m(self, input, model, background, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1869\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1897\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m AsyncStream[ResponseStreamEvent]:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1899\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/responses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1900\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1901\u001b[0m             {\n\u001b[0;32m   1902\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1903\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1904\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m: background,\n\u001b[0;32m   1905\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m: include,\n\u001b[0;32m   1906\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: instructions,\n\u001b[0;32m   1907\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[0;32m   1908\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1909\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1910\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: previous_response_id,\n\u001b[0;32m   1911\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning,\n\u001b[0;32m   1912\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1913\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1914\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1915\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1916\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[0;32m   1917\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1918\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1919\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1920\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m: truncation,\n\u001b[0;32m   1921\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1922\u001b[0m             },\n\u001b[0;32m   1923\u001b[0m             response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParamsStreaming\n\u001b[0;32m   1924\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   1925\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParamsNonStreaming,\n\u001b[0;32m   1926\u001b[0m         ),\n\u001b[0;32m   1927\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1928\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1929\u001b[0m         ),\n\u001b[0;32m   1930\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mResponse,\n\u001b[0;32m   1931\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1932\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ResponseStreamEvent],\n\u001b[0;32m   1933\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\openai\\_base_client.py:1748\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1745\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1746\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1747\u001b[0m )\n\u001b[1;32m-> 1748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\openai\\_base_client.py:1555\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Too many images in request. Max is 50.', 'type': None, 'param': None, 'code': 'BadRequest'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAgentExecutionException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚠️ Parse error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m classify()\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mget_response(messages\u001b[38;5;241m=\u001b[39m[user_msg])\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:69\u001b[0m, in \u001b[0;36mtrace_agent_get_response.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mdescription:\n\u001b[0;32m     67\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attribute(gen_ai_attributes\u001b[38;5;241m.\u001b[39mAGENT_DESCRIPTION, agent\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_response_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\semantic_kernel\\agents\\open_ai\\openai_responses_agent.py:900\u001b[0m, in \u001b[0;36mOpenAIResponsesAgent.get_response\u001b[1;34m(self, messages, thread, arguments, kernel, include, instruction_role, instructions_override, function_choice_behavior, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, text, tools, temperature, top_p, truncation, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m function_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[0;32m    899\u001b[0m response_messages: \u001b[38;5;28mlist\u001b[39m[ChatMessageContent] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 900\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m is_visible, response \u001b[38;5;129;01min\u001b[39;00m ResponsesAgentThreadActions\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    901\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    902\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[0;32m    903\u001b[0m     thread\u001b[38;5;241m=\u001b[39mthread,\n\u001b[0;32m    904\u001b[0m     store_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_enabled,\n\u001b[0;32m    905\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m    906\u001b[0m     arguments\u001b[38;5;241m=\u001b[39marguments,\n\u001b[0;32m    907\u001b[0m     function_choice_behavior\u001b[38;5;241m=\u001b[39mfunction_choice_behavior,\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_level_params,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    909\u001b[0m ):\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_visible \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    911\u001b[0m         response\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m thread\u001b[38;5;241m.\u001b[39mid\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\semantic_kernel\\agents\\open_ai\\responses_agent_thread_actions.py:176\u001b[0m, in \u001b[0;36mResponsesAgentThreadActions.invoke\u001b[1;34m(cls, agent, chat_history, thread, store_enabled, function_choice_behavior, arguments, include, instructions_override, kernel, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, text, tools, temperature, top_p, truncation, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     previous_response_id \u001b[38;5;241m=\u001b[39m thread\u001b[38;5;241m.\u001b[39mresponse_id\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(function_choice_behavior\u001b[38;5;241m.\u001b[39mmaximum_auto_invoke_attempts):\n\u001b[1;32m--> 176\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[0;32m    177\u001b[0m         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    178\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39moverride_history,\n\u001b[0;32m    179\u001b[0m         merged_instructions\u001b[38;5;241m=\u001b[39mmerged_instructions,\n\u001b[0;32m    180\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    181\u001b[0m         store_output_enabled\u001b[38;5;241m=\u001b[39mstore_enabled,\n\u001b[0;32m    182\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    183\u001b[0m         response_options\u001b[38;5;241m=\u001b[39mresponse_options,\n\u001b[0;32m    184\u001b[0m     )\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Response):\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AgentInvokeException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse is not of type Response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\notanaha\\AppData\\Local\\anaconda3\\envs\\py12\\Lib\\site-packages\\semantic_kernel\\agents\\open_ai\\responses_agent_thread_actions.py:510\u001b[0m, in \u001b[0;36mResponsesAgentThreadActions._get_response\u001b[1;34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[0;32m    507\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m encountered a content error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    508\u001b[0m             ex,\n\u001b[0;32m    509\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed to complete the request\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    512\u001b[0m         ex,\n\u001b[0;32m    513\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the request\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    517\u001b[0m         ex,\n\u001b[0;32m    518\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mAgentExecutionException\u001b[0m: (\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'message': 'Too many images in request. Max is 50.', 'type': None, 'param': None, 'code': 'BadRequest'}}\"))"
     ]
    }
   ],
   "source": [
    "async def classify():\n",
    "    response = await agent.get_response(messages=[user_msg])\n",
    "    print(\"Raw output:\\n\", response.content)\n",
    "    try:\n",
    "        result = Step5Response.model_validate_json(response.content)\n",
    "        print(\"\\nParsed:\", result)\n",
    "    except Exception as e:\n",
    "        print(\"\\n⚠️ Parse error:\", e)\n",
    "\n",
    "await classify()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
